{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.aqua.operators import Z, Y, X\n",
    "from qiskit.aqua.operators import StateFn\n",
    "\n",
    "QUBITS = 4\n",
    "operatorZ = Z ^ Z ^ Z ^ Z\n",
    "operatorX = X ^ X ^ X ^ X\n",
    "operatorY = Y ^ Y ^ Y ^ Y\n",
    "\n",
    "def quantum_layer(initial_parameters):\n",
    "    # expecting parameters to be a numpy array\n",
    "    quantumRegister = QuantumRegister(QUBITS)\n",
    "    quantumCircuit = QuantumCircuit(quantumRegister)\n",
    "    \n",
    "    quantumCircuit.h(range(4))\n",
    "\n",
    "    for i in range(len(initial_parameters)):\n",
    "      quantumCircuit.ry(initial_parameters[i] * np.pi, i)\n",
    "   \n",
    "    psi = StateFn(quantumCircuit)\n",
    "    \n",
    "    # two ways of doing the same thing\n",
    "    expectationX = (~psi @ operatorX @ psi).eval()\n",
    "    expectationZ = psi.adjoint().compose(operatorZ).compose(psi).eval().real\n",
    "    expectationY = (~psi @ operatorY @ psi).eval()\n",
    "    \n",
    "    expectationZ = np.abs(np.real(expectationZ))\n",
    "    expectations = [expectationX, expectationY, expectationZ, \n",
    "                    expectationX + expectationY + expectationZ] \n",
    "\n",
    "    return np.array(expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, batch_size=64, units=4,**kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        final_output = []\n",
    "        for i in range(inputs.shape[0]):\n",
    "          pred = quantum_layer(inputs[i].numpy())\n",
    "          final_output.append(list(pred))\n",
    "        return tf.convert_to_tensor(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import tensorflow as tf\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "class Mlp(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            layer_sizes,\n",
    "            output_size=None,\n",
    "            activations=None,\n",
    "            output_activation=None,\n",
    "            use_bias=True,\n",
    "            kernel_initializer=None,\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=None,\n",
    "            bias_regularizer=None,\n",
    "            activity_regularizer=None,\n",
    "            kernel_constraint=None,\n",
    "            bias_constraint=None,\n",
    "            trainable=True,\n",
    "            name=None,\n",
    "            name_internal_layers=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Stacks len(layer_sizes) dense layers on top of each other, with an additional layer with output_size neurons,\n",
    "         if specified.\n",
    "         \"\"\"\n",
    "        self.layers = []\n",
    "        internal_name = None\n",
    "        # If object isn't a list, assume it is a single value that will be repeated for all values\n",
    "        if not isinstance(activations, list):\n",
    "            activations = [activations for _ in layer_sizes]\n",
    "        # end if\n",
    "        # If there is one specifically for the output, add it to the list of layers to be built\n",
    "        if output_size is not None:\n",
    "            layer_sizes = layer_sizes + [output_size]\n",
    "            activations = activations + [output_activation]\n",
    "        # end if\n",
    "\n",
    "        new_layer = tf.layers.Dense(\n",
    "            4,\n",
    "            activation='softmax',\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            trainable=trainable,\n",
    "            name=internal_name\n",
    "        )\n",
    "        layer_2 = Linear(new_layer)\n",
    "        layer_3 = Linear(layer_2)\n",
    "\n",
    "        self.layers.append(new_layer)\n",
    "        self.layers.append(layer_2)\n",
    "        self.layers.append(layer_3)\n",
    "\n",
    "    # end __init__\n",
    "\n",
    "    def __call__(self, inputs, *args, **kwargs):\n",
    "        outputs = [inputs]\n",
    "        for layer in self.layers:\n",
    "            outputs.append(layer(outputs[-1]))\n",
    "        # end for\n",
    "        return outputs[-1]\n",
    "    # end __call__\n",
    "# end Mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNN(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        var,\n",
    "        mat,\n",
    "        msg,\n",
    "        loop,\n",
    "        MLP_depth=3,\n",
    "        MLP_weight_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "        MLP_bias_initializer=tf.zeros_initializer,\n",
    "        RNN_cell=tf.compat.v1.nn.rnn_cell.LSTMCell,\n",
    "        Cell_activation=tf.nn.relu,\n",
    "        Msg_activation=tf.nn.relu,\n",
    "        Msg_last_activation=None,\n",
    "        float_dtype=tf.float32,\n",
    "        name='GraphNN'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Receives three dictionaries: var, mat and msg.\n",
    "        ○ var is a dictionary from variable names to embedding sizes.\n",
    "          That is: an entry var[\"V1\"] = 10 means that the variable \"V1\" will have an embedding size of 10.\n",
    "\n",
    "        ○ mat is a dictionary from matrix names to variable pairs.\n",
    "          That is: an entry mat[\"M\"] = (\"V1\",\"V2\") means that the matrix \"M\" can be used to mask messages from \"V1\" to \"V2\".\n",
    "\n",
    "        ○ msg is a dictionary from function names to variable pairs.\n",
    "          That is: an entry msg[\"cast\"] = (\"V1\",\"V2\") means that one can apply \"cast\" to convert messages from \"V1\" to \"V2\".\n",
    "\n",
    "        ○ loop is a dictionary from variable names to lists of dictionaries:\n",
    "          {\n",
    "            \"mat\": the matrix name which will be used,\n",
    "            \"transpose?\": if true then the matrix M will be transposed,\n",
    "            \"fun\": transfer function (python function built using tensorflow operations,\n",
    "            \"msg\": message name,\n",
    "            \"var\": variable name\n",
    "          }\n",
    "          If \"mat\" is None, it will be the identity matrix,\n",
    "          If \"transpose?\" is None, it will default to false,\n",
    "          if \"fun\" is None, no function will be applied,\n",
    "          If \"msg\" is false, no message conversion function will be applied,\n",
    "          If \"var\" is false, then [1] will be supplied as a surrogate.\n",
    "\n",
    "          That is: an entry loop[\"V2\"] = [ {\"mat\":None,\"fun\":f,\"var\":\"V2\"}, {\"mat\":\"M\",\"transpose?\":true,\"msg\":\"cast\",\"var\":\"V1\"} ] enforces the following update rule for every timestep:\n",
    "            V2 ← tf.append( [ f(V2), Mᵀ × cast(V1) ] )\n",
    "        \"\"\"\n",
    "        self.var, self.mat, self.msg, self.loop, self.name = var, mat, msg, loop, name\n",
    "\n",
    "        self.MLP_depth = MLP_depth\n",
    "        self.MLP_weight_initializer = MLP_weight_initializer\n",
    "        self.MLP_bias_initializer = MLP_bias_initializer\n",
    "        self.RNN_cell = RNN_cell\n",
    "        self.Cell_activation = Cell_activation\n",
    "        self.Msg_activation = Msg_activation\n",
    "        self.Msg_last_activation = Msg_last_activation\n",
    "        self.float_dtype = float_dtype\n",
    "\n",
    "        # Check model for inconsistencies\n",
    "        self.check_model()\n",
    "\n",
    "        # Initialize the parameters\n",
    "        with tf.compat.v1.variable_scope(self.name):\n",
    "            with tf.compat.v1.variable_scope('parameters'):\n",
    "                self._init_parameters()\n",
    "            # end parameter scope\n",
    "        # end GraphNN scope\n",
    "    # end __init__\n",
    "\n",
    "    def check_model(self):\n",
    "        # Procedure to check model for inconsistencies\n",
    "        for v in self.var:\n",
    "            if v not in self.loop:\n",
    "                raise Warning(\n",
    "                    'Variable {v} is not updated anywhere! Consider removing it from the model'.format(v=v))\n",
    "            # end if\n",
    "        # end for\n",
    "\n",
    "        for v in self.loop:\n",
    "            if v not in self.var:\n",
    "                raise Exception(\n",
    "                    'Updating variable {v}, which has not been declared!'.format(v=v))\n",
    "            # end if\n",
    "        # end for\n",
    "\n",
    "        for mat, (v1, v2) in self.mat.items():\n",
    "            if v1 not in self.var:\n",
    "                raise Exception(\n",
    "                    'Matrix {mat} definition depends on undeclared variable {v}'.format(mat=mat, v=v1))\n",
    "            # end if\n",
    "            if v2 not in self.var and type(v2) is not int:\n",
    "                raise Exception(\n",
    "                    'Matrix {mat} definition depends on undeclared variable {v}'.format(mat=mat, v=v2))\n",
    "            # end if\n",
    "        # end for\n",
    "\n",
    "        for msg, (v1, v2) in self.msg.items():\n",
    "            if v1 not in self.var:\n",
    "                raise Exception(\n",
    "                    'Message {msg} maps from undeclared variable {v}'.format(msg=msg, v=v1))\n",
    "            # end if\n",
    "            if v2 not in self.var:\n",
    "                raise Exception(\n",
    "                    'Message {msg} maps to undeclared variable {v}'.format(msg=msg, v=v2))\n",
    "            # end if\n",
    "        # end for\n",
    "    # end check_model\n",
    "\n",
    "    def _init_parameters(self):\n",
    "        # Init LSTM cells\n",
    "        self._RNN_cells = {\n",
    "            v: self.RNN_cell(\n",
    "                d,\n",
    "                activation=self.Cell_activation\n",
    "            ) for (v, d) in self.var.items()\n",
    "        }\n",
    "        # Init message-computing MLPs\n",
    "        self._msg_MLPs = {\n",
    "            msg: Mlp(\n",
    "                layer_sizes=[self.var[vin] for _ in range(self.MLP_depth)],\n",
    "                output_size=self.var[vout],\n",
    "                activations=[\n",
    "                    self.Msg_activation for _ in range(self.MLP_depth)],\n",
    "                output_activation=self.Msg_last_activation,\n",
    "                use_bias=True,\n",
    "                kernel_initializer=self.MLP_weight_initializer(),\n",
    "                bias_initializer=self.MLP_weight_initializer(),\n",
    "                name=msg,\n",
    "                name_internal_layers=True,\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None,\n",
    "                trainable=True,\n",
    "            ) for msg, (vin, vout) in self.msg.items()\n",
    "        }\n",
    "    # end _init_parameters\n",
    "\n",
    "    def __call__(self, adjacency_matrices, initial_embeddings, time_steps, LSTM_initial_states={}):\n",
    "        with tf.compat.v1.variable_scope(self.name):\n",
    "            with tf.compat.v1.variable_scope(\"assertions\"):\n",
    "                assertions = self.check_run(\n",
    "                    adjacency_matrices, initial_embeddings, time_steps, LSTM_initial_states)\n",
    "            # end assertion variable scope\n",
    "            with tf.control_dependencies(assertions):\n",
    "                states = {}\n",
    "                for v, init in initial_embeddings.items():\n",
    "                    h0 = init\n",
    "                    c0 = tf.zeros_like(\n",
    "                        h0, dtype=self.float_dtype) if v not in LSTM_initial_states else LSTM_initial_states[v]\n",
    "                    states[v] = tf.compat.v1.rnn.LSTMStateTuple(h=h0, c=c0)\n",
    "                # end\n",
    "\n",
    "                # Build while loop body function\n",
    "                def while_body(t, states):\n",
    "                    new_states = {}\n",
    "                    for v in self.var:\n",
    "                        inputs = []\n",
    "                        for update in self.loop[v]:\n",
    "                            if 'var' in update:\n",
    "                                y = states[update['var']].h\n",
    "                                if 'fun' in update:\n",
    "                                    y = update['fun'](y)\n",
    "                                # end if\n",
    "                                if 'msg' in update:\n",
    "                                    y = self._msg_MLPs[update['msg']](y)\n",
    "                                # end if\n",
    "                                if 'mat' in update:\n",
    "                                    y = tf.matmul(\n",
    "                                        adjacency_matrices[update['mat']],\n",
    "                                        y,\n",
    "                                        adjoint_a=update['transpose?'] if 'transpose?' in update else False\n",
    "                                    )\n",
    "                                # end if\n",
    "                                inputs.append(y)\n",
    "                            else:\n",
    "                                inputs.append(\n",
    "                                    adjacency_matrices[update['mat']])\n",
    "                            # end if var in update\n",
    "                        # end for update in loop\n",
    "                        inputs = tf.concat(inputs, axis=1)\n",
    "                        with tf.compat.v1.variable_scope('{v}_cell'.format(v=v)):\n",
    "                            _, new_states[v] = self._RNN_cells[v](\n",
    "                                inputs=inputs, state=states[v])\n",
    "                        # end cell scope\n",
    "                    # end for v in var\n",
    "                    return (t+1), new_states\n",
    "                # end while_body\n",
    "\n",
    "                _, last_states = tf.while_loop(\n",
    "                    lambda t, states: tf.less(t, time_steps),\n",
    "                    while_body,\n",
    "                    [0, states]\n",
    "                )\n",
    "            # end assertions\n",
    "        # end Graph scope\n",
    "        return last_states\n",
    "    # end __call__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "def build_network(d):\n",
    "\n",
    "    # Define hyperparameters\n",
    "    d = d\n",
    "    learning_rate = 2e-5\n",
    "    l2norm_scaling = 1e-10\n",
    "    global_norm_gradient_clipping_ratio = 0.65\n",
    "    \n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "    # Placeholder for answers to the decision problems (one per problem)\n",
    "    cn_exists = tf.compat.v1.placeholder( tf.float32, shape = (None,), name = 'cn_exists' )\n",
    "    # Placeholders for the list of number of vertices and edges per instance\n",
    "    n_vertices  = tf.compat.v1.placeholder( tf.int32, shape = (None,), name = 'n_vertices')\n",
    "    n_edges     = tf.compat.v1.placeholder( tf.int32, shape = (None,), name = 'n_edges')\n",
    "    # Placeholder for the adjacency matrix connecting each vertex to its neighbors \n",
    "    M_matrix   = tf.compat.v1.placeholder( tf.float32, shape = (None,None), name = \"M\" )\n",
    "    # Placeholder for the adjacency matrix connecting each vertex to its candidate colors\n",
    "    VC_matrix = tf.compat.v1.placeholder( tf.float32, shape = (None,None), name = \"VC\" )\n",
    "    # Placeholder for chromatic number (one per problem)\n",
    "    chrom_number = tf.compat.v1.placeholder( tf.float32, shape = (None,), name = \"chrom_number\" )\n",
    "    # Placeholder for the number of timesteps the GNN is to run for\n",
    "    time_steps  = tf.compat.v1.placeholder( tf.int32, shape = (), name = \"time_steps\" )\n",
    "    #Placeholder for initial color embeddings for the given batch\n",
    "    colors_initial_embeddings = tf.compat.v1.placeholder( tf.float32, shape=(None,d), name= \"colors_initial_embeddings\")\n",
    "    \n",
    "    \n",
    "    # All vertex embeddings are initialized with the same value, which is a trained parameter learned by the network\n",
    "    total_n = tf.shape(input=M_matrix)[1]\n",
    "    v_init = tf.compat.v1.get_variable(initializer=tf.random.normal((1,d)), dtype=tf.float32, name='V_init')\n",
    "    vertex_initial_embeddings = tf.tile(\n",
    "        tf.compat.v1.div(v_init, tf.sqrt(tf.cast(d, tf.float32))),\n",
    "        [total_n, 1]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define GNN dictionary\n",
    "    GNN = {}\n",
    "\n",
    "    # Define Graph neural network\n",
    "    gnn = GraphNN(\n",
    "        {\n",
    "            # V is the set of vertex embeddings\n",
    "            'V': d,\n",
    "            # C is for color embeddings\n",
    "            'C': d\n",
    "        },\n",
    "        {\n",
    "            # M is a V×V adjacency matrix connecting each vertex to its neighbors\n",
    "            'M': ('V','V'),\n",
    "            # MC is a VxC adjacency matrix connecting each vertex to its candidate colors\n",
    "            'VC': ('V','C')\n",
    "        },\n",
    "        {\n",
    "            # V_msg_C is a MLP which computes messages from vertex embeddings to color embeddings\n",
    "            'V_msg_C': ('V','C'),\n",
    "            # C_msg_V is a MLP which computes messages from color embeddings to vertex embeddings\n",
    "            'C_msg_V': ('C','V')\n",
    "        },\n",
    "        {   # V(t+1) <- Vu( M x V, VC x CmsgV(C) )\n",
    "            'V': [\n",
    "                {\n",
    "                    'mat': 'M',\n",
    "                    'var': 'V'\n",
    "                },\n",
    "                {\n",
    "                    'mat': 'VC',\n",
    "                    'var': 'C',\n",
    "                    'msg': 'C_msg_V'\n",
    "                }\n",
    "            ],\n",
    "            # C(t+1) <- Cu( VC^T x VmsgC(V))\n",
    "            'C': [\n",
    "                {\n",
    "                    'mat': 'VC',\n",
    "                    'msg': 'V_msg_C',\n",
    "                    'transpose?': True,\n",
    "                    'var': 'V'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        ,\n",
    "        name='graph-coloring'\n",
    "    )\n",
    "\n",
    "    # Populate GNN dictionary\n",
    "    GNN['gnn']          = gnn\n",
    "    GNN['cn_exists'] = cn_exists\n",
    "    GNN['n_vertices']   = n_vertices\n",
    "    GNN['n_edges']      = n_edges\n",
    "    GNN[\"M\"]           = M_matrix\n",
    "    GNN[\"VC\"] = VC_matrix\n",
    "    GNN[\"chrom_number\"] = chrom_number\n",
    "    GNN[\"time_steps\"]   = time_steps\n",
    "    GNN[\"colors_initial_embeddings\"] = colors_initial_embeddings\n",
    "\n",
    "    # Define V_vote, which will compute one logit for each vertex\n",
    "    V_vote_MLP = Mlp(\n",
    "        layer_sizes = [ d for _ in range(3) ],\n",
    "        activations = [ tf.nn.relu for _ in range(3) ],\n",
    "        output_size = 1,\n",
    "        name = 'V_vote',\n",
    "        kernel_initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "        bias_initializer = tf.compat.v1.zeros_initializer()\n",
    "        )\n",
    "    \n",
    "    # Get the last embeddings\n",
    "    last_states = gnn(\n",
    "      { \"M\": M_matrix, \"VC\": VC_matrix, 'chrom_number': chrom_number },\n",
    "      { \"V\": vertex_initial_embeddings, \"C\": colors_initial_embeddings },\n",
    "      time_steps = time_steps\n",
    "    )\n",
    "    GNN[\"last_states\"] = last_states\n",
    "    V_n = last_states['V'].h\n",
    "    C_n = last_states['C'].h\n",
    "    \n",
    "    \n",
    "    # Compute a vote for each embedding\n",
    "    V_vote = tf.reshape(V_vote_MLP(V_n), [-1])\n",
    "\n",
    "    # Compute the number of problems in the batch\n",
    "    num_problems = tf.shape(input=n_vertices)[0]\n",
    "\n",
    "    # Compute a logit probability for each problem\n",
    "    pred_logits = tf.while_loop(\n",
    "        cond=lambda i, pred_logits: tf.less(i, num_problems),\n",
    "        body=lambda i, pred_logits:\n",
    "            (\n",
    "                (i+1),\n",
    "                pred_logits.write(\n",
    "                    i,\n",
    "                    tf.reduce_mean(input_tensor=V_vote[tf.reduce_sum(input_tensor=n_vertices[0:i]):tf.reduce_sum(input_tensor=n_vertices[0:i])+n_vertices[i]])\n",
    "                )\n",
    "            ),\n",
    "        loop_vars=[0, tf.TensorArray(size=num_problems, dtype=tf.float32)]\n",
    "        )[1].stack()\n",
    "    # Convert logits into probabilities\n",
    "    GNN['predictions'] = tf.sigmoid(pred_logits)\n",
    "\n",
    "    # Compute True Positives, False Positives, True Negatives, False Negatives, accuracy\n",
    "    GNN['TP'] = tf.reduce_sum(input_tensor=tf.multiply(cn_exists, tf.cast(tf.equal(cn_exists, tf.round(GNN['predictions'])), tf.float32)))\n",
    "    GNN['FP'] = tf.reduce_sum(input_tensor=tf.multiply(cn_exists, tf.cast(tf.not_equal(cn_exists, tf.round(GNN['predictions'])), tf.float32)))\n",
    "    GNN['TN'] = tf.reduce_sum(input_tensor=tf.multiply(tf.ones_like(cn_exists)-cn_exists, tf.cast(tf.equal(cn_exists, tf.round(GNN['predictions'])), tf.float32)))\n",
    "    GNN['FN'] = tf.reduce_sum(input_tensor=tf.multiply(tf.ones_like(cn_exists)-cn_exists, tf.cast(tf.not_equal(cn_exists, tf.round(GNN['predictions'])), tf.float32)))\n",
    "    GNN['acc'] = tf.reduce_mean(input_tensor=tf.cast(tf.equal(cn_exists, tf.round(GNN['predictions'])), tf.float32))\n",
    "\n",
    "    # Define loss\n",
    "    GNN['loss'] = tf.reduce_mean(input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(labels=cn_exists, logits=pred_logits))\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(name='Adam', learning_rate=learning_rate)\n",
    "\n",
    "    # Compute cost relative to L2 normalization\n",
    "    vars_cost = tf.add_n([ tf.nn.l2_loss(var) for var in tf.compat.v1.trainable_variables() ])\n",
    "\n",
    "    # Define gradients and train step\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(ys=GNN['loss'] + tf.multiply(vars_cost, l2norm_scaling),xs=tf.compat.v1.trainable_variables()),global_norm_gradient_clipping_ratio)\n",
    "    GNN['train_step'] = optimizer.apply_gradients(zip(grads, tf.compat.v1.trainable_variables()))\n",
    "    \n",
    "    GNN['C_n'] = C_n\n",
    "    \n",
    "    # Return GNN dictionary\n",
    "    return GNN\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys, os, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_weights(sess, path, scope=None):\n",
    "    if os.path.exists(path):\n",
    "        # Restore saved weights\n",
    "        print(\"Restoring saved model ... \")\n",
    "        # Create model saver\n",
    "        if scope is None:\n",
    "            saver = tf.train.Saver()\n",
    "        else:\n",
    "            saver = tf.train.Saver(var_list=tf.get_collection(\n",
    "                tf.GraphKeys.GLOBAL_VARIABLES, scope=scope))\n",
    "        # end\n",
    "        saver.restore(sess, \"%s/model.ckpt\" % path)\n",
    "    else:\n",
    "        raise Exception('Path does not exist!')\n",
    "    # end if\n",
    "# end\n",
    "\n",
    "\n",
    "def save_weights(sess, path, scope=None):\n",
    "    # Create /tmp/ directory to save weights\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    # end if\n",
    "    # Create model saver\n",
    "    if scope is None:\n",
    "        saver = tf.train.Saver()\n",
    "    else:\n",
    "        saver = tf.train.Saver(var_list=tf.get_collection(\n",
    "            tf.GraphKeys.GLOBAL_VARIABLES, scope=scope))\n",
    "    # end\n",
    "    saver.save(sess, \"%s/model.ckpt\" % path)\n",
    "    print(\"MODEL SAVED IN PATH: {path}\\n\".format(path=path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-2e6d0da041b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building model ...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m \u001b[0mGNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;31m# Comment the following line to allow GPU use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-df28aabe42a6>\u001b[0m in \u001b[0;36mbuild_network\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m     87\u001b[0m         }\n\u001b[1;32m     88\u001b[0m         \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'graph-coloring'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-64e28970ea8f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var, mat, msg, loop, MLP_depth, MLP_weight_initializer, MLP_bias_initializer, RNN_cell, Cell_activation, Msg_activation, Msg_last_activation, float_dtype, name)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parameters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;31m# end parameter scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# end GraphNN scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-64e28970ea8f>\u001b[0m in \u001b[0;36m_init_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             ) for msg, (vin, vout) in self.msg.items()\n\u001b[0m\u001b[1;32m    135\u001b[0m         }\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m# end _init_parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-64e28970ea8f>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             ) for msg, (vin, vout) in self.msg.items()\n\u001b[0m\u001b[1;32m    135\u001b[0m         }\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m# end _init_parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'shape'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys, os, time, random, argparse, timeit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from functools import reduce\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "class InstanceLoader(object):\n",
    "\n",
    "    def __init__(self,path):\n",
    "        self.path = path\n",
    "        self.filenames = [ path + '/' + x for x in os.listdir(path) ]\n",
    "        random.shuffle(self.filenames)\n",
    "        self.reset()\n",
    "    #end\n",
    "\n",
    "    def get_instances(self, n_instances):\n",
    "        for i in range(n_instances):\n",
    "            # Read graph from file\n",
    "            Ma,chrom_number,diff_edge = read_graph(self.filenames[self.index])\n",
    "            f = self.filenames[self.index]\n",
    "            \n",
    "            Ma1 = Ma\n",
    "            Ma2 = Ma.copy()\n",
    "\n",
    "            if diff_edge is not None:\n",
    "                # Create a (UNSAT/SAT) pair of instances with one edge of difference\n",
    "                # The second instance has one edge more (diff_edge) which renders it SAT\n",
    "                Ma2[diff_edge[0],diff_edge[1]] = Ma2[diff_edge[1],diff_edge[0]] =1\n",
    "            #end\n",
    "\n",
    "            # Yield both instances\n",
    "            yield Ma1,chrom_number,f\n",
    "            yield Ma2,chrom_number,f\n",
    "\n",
    "            if self.index + 1 < len(self.filenames):\n",
    "                self.index += 1\n",
    "            else:\n",
    "                self.reset()\n",
    "        #end\n",
    "    #end\n",
    "\n",
    "    def create_batch(instances):\n",
    "\n",
    "        # n_instances: number of instances\n",
    "        n_instances = len(instances)\n",
    "        \n",
    "        # n_vertices[i]: number of vertices in the i-th instance\n",
    "        n_vertices  = np.array([ x[0].shape[0] for x in instances ])\n",
    "        # n_edges[i]: number of edges in the i-th instance\n",
    "        n_edges     = np.array([ len(np.nonzero(x[0])[0]) for x in instances ])\n",
    "        # n_colors[i]: number of colors in the i-th instance\n",
    "        n_colors = np.array( [x[1] for x in instances])\n",
    "        # total_vertices: total number of vertices among all instances\n",
    "        total_vertices  = sum(n_vertices)\n",
    "        # total_edges: total number of edges among all instances\n",
    "        total_edges     = sum(n_edges)\n",
    "        # total_colors: total number of colors among all instances\n",
    "        total_colors = sum(n_colors)\n",
    "\n",
    "        # Compute matrices M, MC\n",
    "        # M is the adjacency matrix\n",
    "        M              = np.zeros((total_vertices,total_vertices))\n",
    "        # MC is a matrix connecting each problem nodes to its colors candidates\n",
    "        MC = np.zeros((total_vertices, total_colors))        \n",
    "\n",
    "        # Even index instances are SAT, odd are UNSAT\n",
    "        cn_exists = np.array([ 1-(i%2) for i in range(n_instances) ])\n",
    "\n",
    "        for (i,(Ma,chrom_number,f)) in enumerate(instances):\n",
    "            # Get the number of vertices (n) and edges (m) in this graph\n",
    "            n, m, c = n_vertices[i], n_edges[i], n_colors[i]\n",
    "            # Get the number of vertices (n_acc) and edges (m_acc) up until the i-th graph\n",
    "            n_acc = sum(n_vertices[0:i])\n",
    "            m_acc = sum(n_edges[0:i])\n",
    "            c_acc = sum(n_colors[0:i])\n",
    "            #Populate MC\n",
    "            MC[n_acc:n_acc+n,c_acc:c_acc+c] = 1\n",
    "\n",
    "            # Get the list of edges in this graph\n",
    "            edges = list(zip(np.nonzero(Ma)[0], np.nonzero(Ma)[1]))\n",
    "\n",
    "            # Populate M\n",
    "            for e,(x,y) in enumerate(edges):\n",
    "                if Ma[x,y] == 1:\n",
    "                  M[n_acc+x,n_acc+y] = M[n_acc+y,n_acc+x] = 1\n",
    "                #end if\n",
    "            #end for\n",
    "        #end for\n",
    "        return M, n_colors, MC, cn_exists, n_vertices, n_edges, f\n",
    "    #end\n",
    "\n",
    "    def get_batches(self, batch_size):\n",
    "        for i in range( len(self.filenames) // batch_size ):\n",
    "            instances = list(self.get_instances(batch_size))\n",
    "            yield InstanceLoader.create_batch(instances)\n",
    "        #end\n",
    "    #end\n",
    "    \n",
    "    def get_test_batches(self, batch_size, total_instances):\n",
    "        for i in range( total_instances ):\n",
    "            instances = list(self.get_instances(batch_size))\n",
    "            yield InstanceLoader.create_batch(instances)\n",
    "        #end\n",
    "    #end\n",
    "\n",
    "    def reset(self):\n",
    "        random.shuffle(self.filenames)\n",
    "        self.index = 0\n",
    "    #end\n",
    "#end\n",
    "\n",
    "def read_graph(filepath):\n",
    "    with open(filepath,\"r\") as f:\n",
    "\n",
    "        line = ''\n",
    "\n",
    "        # Parse number of vertices\n",
    "        while 'DIMENSION' not in line: line = f.readline();\n",
    "        n = int(line.split()[1])\n",
    "        Ma = np.zeros((n,n),dtype=int)\n",
    "        \n",
    "        # Parse edges\n",
    "        while 'EDGE_DATA_SECTION' not in line: line = f.readline();\n",
    "        line = f.readline()\n",
    "        while '-1' not in line:\n",
    "            i,j = [ int(x) for x in line.split() ]\n",
    "            Ma[i,j] = 1\n",
    "            line = f.readline()\n",
    "        #end while\n",
    "\n",
    "        # Parse diff edge\n",
    "        while 'DIFF_EDGE' not in line: line = f.readline();\n",
    "        diff_edge = [ int(x) for x in f.readline().split() ]\n",
    "\n",
    "        # Parse target cost\n",
    "        while 'CHROM_NUMBER' not in line: line = f.readline();\n",
    "        chrom_number = int(f.readline().strip())\n",
    "\n",
    "    #end\n",
    "    return Ma,chrom_number,diff_edge\n",
    "#end\n",
    "\n",
    "\n",
    "def run_training_batch(sess, model, batch, batch_i, epoch_i, time_steps, d, verbose=True):\n",
    "    M, C, VC, cn_exists, n_vertices, n_edges, f = batch\n",
    "    # Generate colors embeddings\n",
    "    ncolors = np.sum(C)\n",
    "    # We define the colors embeddings outside, randomly. They are not learnt by the GNN (that can be improved)\n",
    "    colors_initial_embeddings = np.random.rand(ncolors, d)\n",
    "\n",
    "    # Define feed dict\n",
    "    feed_dict = {\n",
    "        model['M']: M,\n",
    "        model['VC']: VC,\n",
    "        model['chrom_number']: C,\n",
    "        model['time_steps']: time_steps,\n",
    "        model['cn_exists']: cn_exists,\n",
    "        model['n_vertices']: n_vertices,\n",
    "        model['n_edges']: n_edges,\n",
    "        model['colors_initial_embeddings']: colors_initial_embeddings\n",
    "    }\n",
    "\n",
    "    outputs = [model['train_step'], model['loss'], model['acc'], model['predictions'], model['TP'], model['FP'],\n",
    "               model['TN'], model['FN']]\n",
    "\n",
    "    # Run model\n",
    "    loss, acc, predictions, TP, FP, TN, FN = sess.run(outputs, feed_dict=feed_dict)[-7:]\n",
    "\n",
    "    if verbose:\n",
    "        # Print stats\n",
    "        print(\n",
    "            '{train_or_test} Epoch {epoch_i} Batch {batch_i}\\t|\\t(n,m,batch size)=({n},{m},{batch_size})\\t|\\t(Loss,'\n",
    "            'Acc)=({loss:.4f},{acc:.4f})\\t|\\tAvg. (Sat,Prediction)=({avg_sat:.4f},{avg_pred:.4f})'.format(\n",
    "                train_or_test='Train',\n",
    "                epoch_i=epoch_i,\n",
    "                batch_i=batch_i,\n",
    "                loss=loss,\n",
    "                acc=acc,\n",
    "                n=np.sum(n_vertices),\n",
    "                m=np.sum(n_edges),\n",
    "                batch_size=n_vertices.shape[0],\n",
    "                avg_sat=np.mean(cn_exists),\n",
    "                avg_pred=np.mean(np.round(predictions))\n",
    "            ),\n",
    "            flush=True\n",
    "        )\n",
    "    # end\n",
    "    return loss, acc, np.mean(cn_exists), np.mean(predictions), TP, FP, TN, FN\n",
    "\n",
    "\n",
    "# end\n",
    "\n",
    "\n",
    "def run_test_batch(sess, model, batch, batch_i, time_steps, logfile, runtabu=True):\n",
    "    M, n_colors, VC, cn_exists, n_vertices, n_edges, f = batch\n",
    "\n",
    "    # Compute the number of problems\n",
    "    n_problems = n_vertices.shape[0]\n",
    "\n",
    "    # open up the batch, which contains 2 instances\n",
    "    for i in range(n_problems):\n",
    "        n, m, c = n_vertices[i], n_edges[i], n_colors[i]\n",
    "        conn = m / n\n",
    "        n_acc = sum(n_vertices[0:i])\n",
    "        c_acc = sum(n_colors[0:i])\n",
    "\n",
    "        # subset adjacency matrix\n",
    "        M_t = M[n_acc:n_acc + n, n_acc:n_acc + n]\n",
    "        c = c if i % 2 == 0 else c + 1\n",
    "\n",
    "        gnnpred = tabupred = 999\n",
    "        for j in range(2, c + 5):\n",
    "            n_colors_t = j\n",
    "            cn_exists_t = 1 if n_colors_t >= c else 0\n",
    "            VC_t = np.ones((n, n_colors_t))\n",
    "            # Generate colors embeddings\n",
    "            colors_initial_embeddings = np.random.rand(n_colors_t, d)\n",
    "\n",
    "            feed_dict = {\n",
    "                model['M']: M_t,\n",
    "                model['VC']: VC_t,\n",
    "                model['chrom_number']: np.array([n_colors_t]),\n",
    "                model['time_steps']: time_steps,\n",
    "                model['cn_exists']: np.array([cn_exists_t]),\n",
    "                model['n_vertices']: np.array([n]),\n",
    "                model['n_edges']: np.array([m]),\n",
    "                model['colors_initial_embeddings']: colors_initial_embeddings\n",
    "            }\n",
    "\n",
    "            outputs = [model['loss'], model['acc'], model['predictions'], model['TP'], model['FP'], model['TN'],\n",
    "                       model['FN']]\n",
    "\n",
    "            # Run model - chromatic number or more\n",
    "            init_time = timeit.default_timer()\n",
    "            loss, acc, predictions, TP, FP, TN, FN = sess.run(outputs, feed_dict=feed_dict)[-7:]\n",
    "            elapsed_gnn_time = timeit.default_timer() - init_time\n",
    "            gnnpred = n_colors_t if predictions > 0.5 and n_colors_t < gnnpred else gnnpred\n",
    "\n",
    "            # run tabucol\n",
    "            if runtabu:\n",
    "                init_time = timeit.default_timer()\n",
    "                tabu_solution = tabucol(M_t, n_colors_t, max_iterations=1000)\n",
    "                elapsed_tabu_time = timeit.default_timer() - init_time\n",
    "                tabu_sol = 0 if tabu_solution is None else 1\n",
    "                tabupred = n_colors_t if tabu_sol == 1 and n_colors_t < tabupred else tabupred\n",
    "        # end for\n",
    "        logfile.write(\n",
    "            '{batch_i} {i} {n} {m} {conn} {tstloss} {tstacc} {cn_exists} {c} {gnnpred} {prediction} {gnntime} {'\\\n",
    "            'tabupred} {tabutime}\\n'.format(\n",
    "                batch_i=batch_i,\n",
    "                i=i,\n",
    "                n=n,\n",
    "                m=m,\n",
    "                c=c,\n",
    "                conn=conn,\n",
    "                cn_exists=cn_exists_t,\n",
    "                tstloss=loss,\n",
    "                tstacc=acc,\n",
    "                gnnpred=gnnpred,\n",
    "                prediction=predictions.item(),\n",
    "                gnntime=elapsed_gnn_time,\n",
    "                tabupred=tabupred if runtabu else 0,\n",
    "                tabutime=elapsed_tabu_time if runtabu else 0\n",
    "            )\n",
    "        )\n",
    "        logfile.flush()\n",
    "    # end for batch\n",
    "\n",
    "\n",
    "# end\n",
    "\n",
    "def summarize_epoch(epoch_i, loss, acc, sat, pred, train=False):\n",
    "    print(\n",
    "        '{train_or_test} Epoch {epoch_i} Average\\t|\\t(Loss,Acc)=({loss:.4f},{acc:.4f})\\t|\\tAvg. (Sat,'\n",
    "        'Pred)=({avg_sat:.4f},{avg_pred:.4f})'.format(\n",
    "            train_or_test='Train' if train else 'Test',\n",
    "            epoch_i=epoch_i,\n",
    "            loss=np.mean(loss),\n",
    "            acc=np.mean(acc),\n",
    "            avg_sat=np.mean(sat),\n",
    "            avg_pred=np.mean(pred)\n",
    "        ),\n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "\n",
    "# end\n",
    "\n",
    "# Set RNG seed for Python, Numpy and Tensorflow\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.compat.v1.set_random_seed(42)\n",
    "seed = str(42)\n",
    "# Setup parameters\n",
    "d = 64\n",
    "time_steps = 32\n",
    "epochs_n = 10000\n",
    "batch_size = 8\n",
    "\n",
    "loadpath = '.'\n",
    "load_checkpoints = False\n",
    "save_checkpoints = True\n",
    "\n",
    "train_params = {\n",
    "'batches_per_epoch': 128\n",
    "}\n",
    "\n",
    "test_params = {\n",
    "'batches_per_epoch': 1\n",
    "}\n",
    "\n",
    "# Build model\n",
    "print('Building model ...', flush=True)\n",
    "tf.compat.v1.reset_default_graph()\n",
    "GNN = build_network(d)\n",
    "\n",
    "# Comment the following line to allow GPU use\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    # Initialize global variables\n",
    "    print('Initializing global variables ... ', flush=True)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Restore saved weights\n",
    "if load_checkpoints: load_weights(sess, loadpath);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training\n",
    "\n",
    "train_loader = InstanceLoader('adversarial-training')\n",
    "\n",
    "ptrain = 'training_' + seed\n",
    "if not os.path.isdir(ptrain):\n",
    "    os.makedirs(ptrain)\n",
    "with open(ptrain + '/log.dat', 'w') as logfile:\n",
    "    # Run for a number of epochs\n",
    "    for epoch_i in np.arange(epochs_n):\n",
    "\n",
    "        train_loader.reset()\n",
    "\n",
    "        train_stats = {k: np.zeros(train_params['batches_per_epoch']) for k in\n",
    "                       ['loss', 'acc', 'sat', 'pred', 'TP', 'FP', 'TN', 'FN']}\n",
    "\n",
    "        print('Training model...', flush=True)\n",
    "        for (batch_i, batch) in islice(enumerate(train_loader.get_batches(batch_size)),\n",
    "                                       train_params['batches_per_epoch']):\n",
    "            train_stats['loss'][batch_i], train_stats['acc'][batch_i], train_stats['sat'][batch_i], \\\n",
    "            train_stats['pred'][batch_i], train_stats['TP'][batch_i], train_stats['FP'][batch_i], \\\n",
    "            train_stats['TN'][batch_i], train_stats['FN'][batch_i] = run_training_batch(sess, GNN, batch,\n",
    "                                                                                        batch_i, epoch_i,\n",
    "                                                                                        time_steps, d,\n",
    "                                                                                        verbose=True)\n",
    "        # end\n",
    "        summarize_epoch(epoch_i, train_stats['loss'], train_stats['acc'], train_stats['sat'],\n",
    "                        train_stats['pred'], train=True)\n",
    "\n",
    "        # Save weights\n",
    "        savepath = ptrain + '/checkpoints/epoch={epoch}'.format(\n",
    "            epoch=round(200 * np.ceil((epoch_i + 1) / 200)))\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        if save_checkpoints: save_weights(sess, savepath);\n",
    "\n",
    "        logfile.write('{epoch_i} {trloss} {tracc} {trsat} {trpred} {trTP} {trFP} {trTN} {trFN} \\n'.format(\n",
    "\n",
    "            epoch_i=epoch_i,\n",
    "\n",
    "            trloss=np.mean(train_stats['loss']),\n",
    "            tracc=np.mean(train_stats['acc']),\n",
    "            trsat=np.mean(train_stats['sat']),\n",
    "            trpred=np.mean(train_stats['pred']),\n",
    "            trTP=np.mean(train_stats['TP']),\n",
    "            trFP=np.mean(train_stats['FP']),\n",
    "            trTN=np.mean(train_stats['TN']),\n",
    "            trFN=np.mean(train_stats['FN']),\n",
    "\n",
    "        )\n",
    "        )\n",
    "        logfile.flush()\n",
    "    # end\n",
    "    # end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run testing\n",
    "\n",
    "test_loader = InstanceLoader('adversarial-testing')\n",
    "    \n",
    "if not os.path.isdir('testing_' + seed):\n",
    "    os.makedirs('testing_' + seed)\n",
    "with open('testing_' + seed + '/log.dat', 'w') as logfile:\n",
    "\n",
    "    test_loader.reset()\n",
    "    logfile.write(\n",
    "        'batch instance vertices edges connectivity loss acc sat chrom_number gnnpred gnncertainty '\n",
    "        'gnntime tabupred tabutime\\n')\n",
    "    print('Testing model...', flush=True)\n",
    "    for (batch_i, batch) in enumerate(test_loader.get_test_batches(1, 2048)):\n",
    "        run_test_batch(sess, GNN, batch, batch_i, time_steps, logfile, runtabu)\n",
    "    # end\n",
    "    logfile.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
